{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXD3nzm8Ct1o"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k-aLaBEHCrCG"
      },
      "outputs": [],
      "source": [
        "# General Dependencies\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeduFwG_CuOC",
        "outputId": "e841e3ad-267b-4a49-a62d-d7d92d8815fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/502-796-Projects/502/deneme-1\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Run from Colab or local\n",
        "\"\"\"\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    ROOT_PATH = \"/content/gdrive/MyDrive/502-796-Projects/502/deneme-1\"\n",
        "    DATA_PATH = os.path.join(ROOT_PATH, \"Datasets\")\n",
        "    \n",
        "    %cd ./gdrive/MyDrive/502-796-Projects/502/deneme-1\n",
        "\n",
        "except:\n",
        "    ROOT_PATH = os.curdir\n",
        "    DATA_PATH = \"../Datasets\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E7E03tmxKfrK"
      },
      "outputs": [],
      "source": [
        "# Source dependencies\n",
        "from data.dataset import FSSDataset\n",
        "from common.vis import Visualizer\n",
        "from common.evaluation import Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw8PulDqFF0Z"
      },
      "source": [
        "## Prepare PASCAL-$5^i$ Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SMLSQDPuEG2g"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Download PASCAL VOC2012 devkit (train/val data): (uncomment lines below)\n",
        "# ------------------------------------------------------------------------------\n",
        "#!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
        "#!tar -xvf 'VOCtrainval_11-May-2012.tar' -C ./Datasets/ \n",
        "\n",
        "# (or instead of wget, use directly the link to download)\n",
        "# STEP 2: Place \"VOC2012\" folder from downloaded \"VOCdevkit\" folder under a \"Datasets\" folder.\n",
        "# ------------------------------------------------------------------------------\n",
        "#!mv Datasets/VOCdevkit/VOC2012 Datasets/VOC2012\n",
        "\n",
        "# STEP 3: Download extended annotations from here\n",
        "#!wget https://drive.google.com/file/d/10zxG2VExoEZUeyQl_uXga2OWHjGeZaf2/view\n",
        "\n",
        "# STEP 4: Extract Put the downloaded extension under \"Datasets/VOC2012/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_HwSM49MOSM",
        "outputId": "bb442e9a-ad30-495a-deb7-46d97dad395f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total (trn) images are : 11394\n",
            "Total (val) images are : 346\n"
          ]
        }
      ],
      "source": [
        "# Dataset initialization \n",
        "# TODO: does paper mention about image size? TODO: yes, 473\n",
        "# TODO: they also do data aug...\n",
        "# TODO: add disclaimer to dataset files https://github.com/juhongm999/hsnet/blob/main/train.py\n",
        "FSSDataset.initialize(img_size=128, datapath=DATA_PATH, use_original_imgsize=False)\n",
        "dataloader_trn = FSSDataset.build_dataloader(benchmark='pascal', bsz=4, nworker=1, fold=0, split='trn')\n",
        "dataloader_val = FSSDataset.build_dataloader(benchmark='pascal', bsz=4, nworker=1, fold=0, split='val')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVGX4VW3HkTN"
      },
      "source": [
        "## Visualization deneme"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WvVXL82vHiF8"
      },
      "outputs": [],
      "source": [
        "Visualizer.initialize(True)\n",
        "Evaluator.initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXPe83EmgIuM"
      },
      "outputs": [],
      "source": [
        "\"\"\" Code below visualizes prediction (to be used in testing)\n",
        "\n",
        "for idx, batch in enumerate(dataloader_trn):\n",
        "        # 1. Hypercorrelation Squeeze Networks forward pass\n",
        "        #batch = utils.to_cuda(batch)\n",
        "        #pred_mask = model.module.predict_mask_nshot(batch, nshot=nshot)\n",
        "\n",
        "        #assert pred_mask.size() == batch['query_mask'].size()\n",
        "        pred_mask = batch['query_mask']\n",
        "\n",
        "        # 2. Evaluate prediction\n",
        "        area_inter, area_union = Evaluator.classify_prediction(pred_mask.clone(), batch)\n",
        "        \n",
        "        #average_meter.update(area_inter, area_union, batch['class_id'], loss=None)\n",
        "        #average_meter.write_process(idx, len(dataloader), epoch=-1, write_batch_idx=1)\n",
        "\n",
        "        # Visualize predictions\n",
        "        if Visualizer.visualize:\n",
        "            Visualizer.visualize_prediction_batch(batch['support_imgs'], batch['support_masks'],\n",
        "                                                  batch['query_img'], batch['query_mask'],\n",
        "                                                  pred_mask, batch['class_id'], idx,\n",
        "                                                  area_inter[1].float() / area_union[1].float())\n",
        "\n",
        "        break   # TODO: delete this break to run visualization for full dataset\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH7NWkQWcM41"
      },
      "source": [
        "## Extract features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE1heUQXcM43"
      },
      "source": [
        "## Initial Agent Tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cnPlbfYfcM44"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# num_tokens is K in the algorithm 1 (not the K of K-shot images)\n",
        "# Please refer to Algorithm 1 given in Supplementary Material\n",
        "# to match the notation of variables in the comments.\n",
        "# We assume X, locations of foreground pixels, is the locations of\n",
        "# f_s, i..e foreground support pixel features.\n",
        "\n",
        "\n",
        "# Shapes:\n",
        "# X --> [batchsize, num_foreground_pixels, 2] --> edit: since number of foreground pixels change for every image, X is a list with len(X)=batchsize\n",
        "# L --> [batchsize, num_background_pixels, 2] last dimension is (x,y) location\n",
        "# f_s --> [batchsize, h, w, c] \n",
        "# \"h, w denote the height, width of the feature map.\" (Supplementary Material)\n",
        "def init_agent_tokens(num_tokens, X, L, f_s):\n",
        "  \n",
        "    # Compute euclidean distance between every pair\n",
        "    # (foreground_pixel, bacground_pixel)\n",
        "    # in total, |X| x |L| pairs\n",
        "    #dists_batch = torch.cdist(X, L)   # Get all the distances for K support ims\n",
        "\n",
        "    tokens = torch.empty((len(X), num_tokens, f_s.shape[1]))\n",
        "    L_new = []\n",
        "    # TODO: can we compute this jointly for all images in a batch?\n",
        "    for i in range(len(X)):\n",
        "        L_single = L[i]      # L for a single image in a batch\n",
        "\n",
        "        for k in range(num_tokens):\n",
        "            #dists = dists_batch[i]\n",
        "            dists = torch.from_numpy(cdist(X[i], L[i], 'euclidean'))   # Get all the distances for K support ims\n",
        "\n",
        "            # See line 3 of Algorithm 1 in Supplementary Material:\n",
        "            # for a specific location x, min distance between x and all other locations in L\n",
        "            d_x, d_x_ind = torch.min(dists, dim=1)  \n",
        "\n",
        "            # We don't care about the actual distance value, so it is named as _\n",
        "            # we care about which location has the furthest distance p* \n",
        "            _ , p_ind = torch.max(d_x, dim=0)\n",
        "\n",
        "            p_furthest = X[i][p_ind, :]      # This is a location (x,y) of a pixel\n",
        "            p_star = p_furthest.unsqueeze(0) # [2] --> [1,2] \n",
        "            L_single = torch.cat([L_single, p_star], dim=0) # L = (B) U (P), see line 5 in Algorithm 1\n",
        "\n",
        "            f_a_k = f_s[i, :, p_furthest.data[0].long().item(), p_furthest.data[1].long().item()]\n",
        "            \n",
        "            tokens[i,k,:] = f_a_k\n",
        "            \n",
        "        L_new.append(L_single)\n",
        "    \n",
        "    return tokens\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Agent Tokens test (to be removed)"
      ],
      "metadata": {
        "id": "UyB1_L4w43kG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from model.featureextractor import FeatureExtractor\n",
        "\n",
        "Feat = FeatureExtractor()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFOLGDYNgPbP",
        "outputId": "9e2792dc-84f6-4b3b-f704-e393d13c7d7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 176MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3qp44JltcM45"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    tmp_supp_feat  torch.Size([4, 2048, 16, 16])\n",
        "    tmp_mask  torch.Size([4, 1, 16, 16])\n",
        "    tmp_supp  torch.Size([4, 2048, 16, 16])\n",
        "\"\"\"\n",
        "\n",
        "for idx, batch in enumerate(dataloader_trn):\n",
        "    \n",
        "    query_img = batch['query_img']\n",
        "    supp_imgs = batch['support_imgs']\n",
        "    supp_masks = batch['support_masks']\n",
        "    \n",
        "    F_Q, F_S, s_mask_list, f_s, M_s = Feat(query_img, supp_imgs, supp_masks)\n",
        "\n",
        "    #f_s = Feat.fg_supp_pix\n",
        "    #M_s = Feat.feature_supp_masks\n",
        "\n",
        "    #print(f_s.shape)\n",
        "    #print(M_s.shape)\n",
        "    \n",
        "    # get background pixels\n",
        "    # get agent tokens\n",
        "\n",
        "    # TODO: can we get rid of for loop?\n",
        "    X = []\n",
        "    L = []\n",
        "    num_tokens = 15 # TODO: make it a hyperparameter\n",
        "    for i, m in enumerate(M_s):  # M_s has shape (batchsize, 1, 16, 16)\n",
        "      m = m.squeeze(0)  # Get a single mask, shape (16, 16)\n",
        "\n",
        "      fg = np.where(m == 1.) # get foreground pixels\n",
        "      bg = np.where(m == 0.) # get background pixels\n",
        "      \n",
        "      \"\"\"\n",
        "      print(fg[0].shape)\n",
        "      print(fg[1].shape)\n",
        "      print(bg[0].shape)\n",
        "      print(bg[1].shape)\n",
        "      \"\"\"\n",
        "\n",
        "      # Create tensor with shape [num_foreground_pix, 2] where the last dimension has (x,y) locations of foreground pixels\n",
        "      foreground_pix = torch.stack((torch.from_numpy(fg[0]), torch.from_numpy(fg[1])), dim=1)\n",
        "      background_pix = torch.stack((torch.from_numpy(bg[0]), torch.from_numpy(bg[1])), dim=1)\n",
        "\n",
        "      \"\"\"\n",
        "      print(\"\\n-----\\n\")\n",
        "      print(foreground_pix.shape)\n",
        "      print(background_pix.shape)\n",
        "      print(\"\\n=====\\n\")\n",
        "      \"\"\"\n",
        "\n",
        "      X.append(foreground_pix)\n",
        "      L.append(background_pix)\n",
        "\n",
        "\n",
        "    tokens = init_agent_tokens(num_tokens, X, L, f_s) # every token has [K,c] dim for every sample in a batch\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens.shape) # shape [batchsize, num_tokens, c] where c is from support features dimension [batchsize, c, h, w]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ia4DiGAn3zLN",
        "outputId": "5f114a38-dbeb-4b41-f008-05e9908f6c75"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 15, 2048])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}